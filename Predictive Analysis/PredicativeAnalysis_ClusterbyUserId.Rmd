---
title: "InstaCart Predictive Analysis: Clustering and K-Means by `user_id`"
author: "April Leclair, Daniel Ochoa, Hoang Anh Thai Vu, Qiuhan Sun"
date: "November 14, 2017"
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "PredictiveAnalysis_HTML_Output") })
output:
  bookdown::tufte_html2:
    number_sections: no
    split_by: none
    toc: no
  bookdown::html_document2:
    number_sections: no
    split_by: none
    toc: no
  bookdown::tufte_handout2:
    latex_engine: xelatex
    number_sections: no
    toc: no
---

```{r setup, include=FALSE, cache=FALSE}
library(readr)
library(randomForest)
library(gbm)
library(tufte)
library(tidyverse)
library(ggplot2)
library(ggmap)
library(tint)
library(lubridate)
library(tree)               #for classification trees
library(caret)              #for confusion matrices
library(e1071)              #to make caret run
library(rpart)              #alternative package to make trees
library(randomForest)       #to make random forests
library(gbm)                #to make gradient boosted trees
library(ape)                #to make hierarchical clusters
library(mclust)             #for k-means clustering
library(BBmisc)             #for data normalization
knitr::opts_chunk$set(tidy = FALSE, message=FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

```{r, include=FALSE}
# Loading my Workspace
# load("my_work_space.RData")
```

# Loading the Data

```{r message=FALSE, warning=FALSE, cache=TRUE}
load("../Source/orders_sample40.Rda")
load("../Source/order_products_sample40.Rda")
products <- read_csv("../Source/products.csv")
departments <- read_csv("../Source/departments.csv")
aisles <- read_csv("../Source/aisles.csv")
orders <- orders_sample40
order_products <- order_products_sample40
rm(orders_sample40, order_products_sample40)  # remove unnecessary data tables
```

```{r}
names(orders)
names(order_products)
names(products)
names(departments)
names(aisles)
```



# Question 1 : Days since prior "Banana" order for the top Banana orderer: `user_id` == 194931

## 1a) Preparing the Data

### * The Top 10 Banana Orderers

`user_id` 194931 has the highest banana order 
```{r}
orders %>%
  left_join(order_products, by="order_id") %>%
  group_by(user_id) %>%
  left_join(products, by="product_id") %>%
  filter(product_name == "Banana") %>%
  group_by(user_id) %>%
  summarize(banana_count=n()) %>%
  arrange(desc(banana_count)) %>%
  head(n=10)
```

### * Delete NA's and add the `days_since_prior_order` variable 

```{r}
orders %>%
  left_join(order_products, by="order_id") %>%
  group_by(user_id) %>%
  left_join(products, by="product_id") %>%
  filter(product_name == "Banana") %>%
  na.omit() %>%
  group_by(user_id) %>%
  summarize(banana_count=n(),
            days_since_prior_order=mean(days_since_prior_order)) %>%
  arrange(desc(banana_count)) %>%
  head(n=10)
```

```{r}
user194931_banana <- 
  orders %>%
  filter(user_id == 194931) %>%  #	(top "Banana" orderer)
  left_join(order_products, by="order_id") %>%
  left_join(products, by="product_id") %>%
  filter(product_name == "Banana") %>%
  arrange(order_number) 
  
user32971_banana <- 
  orders %>%
  filter(user_id == 32971) %>%  #	(top "Banana" orderer)
  left_join(order_products, by="order_id") %>%
  left_join(products, by="product_id") %>%
  filter(product_name == "Banana") %>%
  arrange(order_number) 
```

## 1b) Feature Engineering

```{r}
# Discrete Factored Time of the Day
new_levels <- c("Late Evening","Late Evening","Late Evening","Dawn","Dawn","Dawn","Dawn",
                "Morning","Morning","Morning","Morning","Morning",
                "Afternoon","Afternoon","Afternoon","Afternoon","Afternoon",
                "Evening","Evening","Evening","Evening",
                "Late Evening","Late Evening","Late Evening","Late Evening")
user194931_banana$timeofday <- factor(new_levels[user194931_banana$order_hour_of_day])
user194931_banana$timeofday <- factor(user194931_banana$timeofday, levels = c("Dawn", "Morning", "Afternoon", "Evening", "Late Evening"))
user32971_banana$timeofday <- factor(new_levels[user32971_banana$order_hour_of_day])
user32971_banana$timeofday <- factor(user32971_banana$timeofday, levels = c("Dawn", "Morning", "Afternoon", "Evening", "Late Evening"))
rm(new_levels)

# Continuous Numeric Time of the Day
user194931_banana$timeofday_num <- as.numeric(user194931_banana$order_hour_of_day)
user32971_banana$timeofday_num <- as.numeric(user32971_banana$order_hour_of_day)

# Ordered Factor Day of the Week
levels(user194931_banana$order_dow) <- c("Sunday", "Monday", "Tuesday", "Wednesday",
                                                 "Thursday", "Friday", "Saturday")
levels(user32971_banana$order_dow) <- c("Sunday", "Monday", "Tuesday", "Wednesday",
                                                 "Thursday", "Friday", "Saturday")
```

## 1c) Visualization: Discrete Time of the Day

user194931 made most of the banana orders in either the morning or the afternoon. The days since prior banana order is in the range of 0 to 7, but most fall 4 or less. There have been more banana orders made in the last half of the order chronology, and it seems that this increased orders with two outliers (overestimates) increased the trendline. The last observation is that there seems to be a serial pattern in days since prior order. For example, the first few banana orders tend to alternate between 0 and 4. The last few seem to alternate between 1 and a little bit over 3 with more upward variability.

```{r, warning=FALSE, fig.width=10, fig.height=7.5}
user194931_banana %>%
  na.omit() %>%
  ggplot(aes(x=order_number, y=days_since_prior_order)) +
  geom_line(color="#FF9999") +
  geom_point(aes(size = 0.5, alpha = 0.25, color=timeofday)) +
  scale_color_brewer(palette = "RdPu") +
  guides(size=FALSE, alpha=FALSE) +
  geom_smooth(stat="smooth", position="stack", method="loess") +
  theme(axis.text.x=element_text(angle=45,hjust=1)) +
  labs(title="Days Since Prior Banana Order for User ID 194931",
       subtitle="by Time of the Day",
       color="Time of the Day",
       size="Size",
       x="Order Chronology", y="Days Since Prior Banana Order",
       caption="Data from InstaCart Kaggle Competition")
```

user32971 made most of the banana orders in the afternoon. The days since prior banana order is in the range of 0 to 5, and most alternate between 2 and 3 (less variability compared to user194931). It seems that the trend line decreased mostly due to the few orders made in 0 or one day in the recent half of the order chronology. The serial alternation still exist but between 2 and 3, as mentioned above. 

```{r, warning=FALSE, fig.width=10, fig.height=7.5}
user32971_banana %>%
  na.omit() %>%
  ggplot(aes(x=order_number, y=days_since_prior_order)) +
  geom_line(color="#FF9999") +
  geom_point(aes(size = 0.5, alpha = 0.25, color=timeofday)) +
  scale_color_brewer(palette = "RdPu") +
  guides(size=FALSE, alpha=FALSE) +
  geom_smooth(stat="smooth", position="stack", method="loess") +
  theme(axis.text.x=element_text(angle=45,hjust=1)) +
  labs(title="Days Since Prior Banana Order for User ID 32971",
       subtitle="by Time of the Day",
       color="Time of the Day",
       size="Size",
       x="Order Chronology", y="Days Since Prior Banana Order",
       caption="Data from InstaCart Kaggle Competition")
```

## 1d) Some More Comparisons Between `user_id` 194931 & 32971

```{r}
# user194931: Most orders were made in the afternoon (53) as we saw above.
table(user194931_banana$timeofday) 

# user32971: Most orders were made in the afternoon (47) as we saw above.
table(user32971_banana$timeofday) 


# user194931: One day (38) was the most common duration till the next order and the next common was four days (13). This explains the 1-4 alternation in the graph.
table(user194931_banana$days_since_prior_order) 

# user32971: Three days (44) was the most common duration till the next order and the next common was two days (25). This explains the 2-3 alternation in the graph.
table(user32971_banana$days_since_prior_order) 


# user194931: The most common `add_to_cart_order` was one (57) and the next common was two (14). This means that bananas are almost always added first or maybe added by default at every order.
a <- table(user194931_banana$add_to_cart_order)  
plot(a)
rm(a)

# user32971: The most common `add_to_cart_order` was 5 or 6 (22, 18), but the data is mostl right-skew most of which fall under 5-6. 
table(user32971_banana$add_to_cart_order)  
plot(b)
rm(b)



# Some more exploration tables

## Average Days Since Prior Order vs. Time of Day
user194931_banana %>% 
  group_by(timeofday) %>% 
  na.omit() %>%
  summarize(avg_days_since=mean(days_since_prior_order)) %>% head(n=10)
user32971_banana %>% 
  group_by(timeofday) %>% 
  na.omit() %>%
  summarize(avg_days_since=mean(days_since_prior_order)) %>% head(n=10)

## Average Days Since Prior Order vs. Hour of Day
user194931_banana %>% 
  group_by(order_hour_of_day) %>% 
  na.omit() %>%
  summarize(avg_days_since=mean(days_since_prior_order)) %>% head(n=10)
user32971_banana %>% 
  group_by(order_hour_of_day) %>% 
  na.omit() %>%
  summarize(avg_days_since=mean(days_since_prior_order)) %>% head(n=10)
```

## 1e) Visualization: Histogram of `order_dow` for `user_id` 194931 & 32971

user194931 made most orders on Saturday at 4pm and 5pm. 

```{r, warning=FALSE, fig.width=10, fig.height=7.5}
user194931_banana %>%
  na.omit() %>%
  group_by(order_dow, order_hour_of_day) %>%
  summarise(count = n()) %>%
  ggplot(aes(x=order_dow, y=count, fill=order_hour_of_day)) +
  geom_bar(stat="identity") +
  theme(axis.text.x=element_text(angle=45,hjust=1)) +
  labs(title="Day of the Week: Banana Orders for User ID 194931",
       subtitle="",
       x="Day of the Week",
       y="Count",
       fill="Order Hour of Day",
       caption="Data from InstaCart Kaggle Competition")
```

user32971 seems to be less variable in their banana orders in terms of day of the week and time of the day. This user also has more spread-out order hour of day (during the afternoon).

```{r, warning=FALSE, fig.width=10, fig.height=7.5}
user32971_banana %>%
  na.omit() %>%
  group_by(order_dow, order_hour_of_day) %>%
  summarise(count = n()) %>%
  ggplot(aes(x=order_dow, y=count, fill=order_hour_of_day)) +
  geom_bar(stat="identity") +
  theme(axis.text.x=element_text(angle=45,hjust=1)) +
  labs(title="Day of the Week: Banana Orders for User ID 32971",
       subtitle="",
       x="Day of the Week",
       y="Count",
       fill="Order Hour of Day",
       caption="Data from InstaCart Kaggle Competition")
```

## 1f) Some More Comparisons Between `user_id` 194931 & 32971

```{r}
# user194931: Most banana orders were made on Saturday (34) and the second highest was Tuesday (21). This explains the alternating pattern between 1 and 4. 
table(user194931_banana$order_dow) 

# user32971: Most banana orders were made on Sunday (19) and the second highest were Tuesday, Wednesday, and Friday (all at 15). This explains the frequent alternation between 2 and 3.  
table(user32971_banana$order_dow) 


# user194931: 9am to 5pm is the range of banana orders. Most banana orders were at 11am (15), 2pm (14), and 4pm (15).
table(user194931_banana$order_hour_of_day) 

# user32971: 8am to 9pm is the range of banana orders. Most banana orders were made between 9am and 11am (10, 14, 11) and at 2pm (16).
table(user32971_banana$order_hour_of_day) 



## Average Time of Day and SD vs. Day of the Week
user194931_banana %>% 
  group_by(order_dow) %>% 
  na.omit() %>%
  summarize(avg_hr_of_day=mean(as.numeric(order_hour_of_day)),
            sd_hr_of_day=sd(as.numeric(order_hour_of_day))) %>% head(n=10)
user32971_banana %>% 
  group_by(order_dow) %>% 
  na.omit() %>%
  summarize(avg_hr_of_day=mean(as.numeric(order_hour_of_day)),
            sd_hr_of_day=sd(as.numeric(order_hour_of_day))) %>% head(n=10)
```

### 1ce) Visualization: Boxplot of `days_since_prior_order`

The days before the next banana order for user194931 is generally between 1.5 and 4.5 with the median value at 3.
```{r, warning=FALSE, fig.width=5, fig.height=5}
user194931_banana %>%
  na.omit() %>%
  group_by(days_since_prior_order) %>%
  summarise(count = n()) %>%
  ggplot(aes(x=count, y=days_since_prior_order)) +
  geom_boxplot(fill="seagreen4") +
  theme(axis.text.x=element_text(angle=45,hjust=1)) +
  labs(title="Days Since Prior Order",
       subtitle="for Banana Orders for User ID 194931",
       x="Days Since Prior Order",
       y="Count",
       fill="Day of the Week",
       caption="Data from InstaCart Kaggle Competition")
```

The days before the next banana order for user32971 is generally between 1.25 and 3.75 (wider range) with the median value at 2.5 (lower median).

```{r, warning=FALSE, fig.width=5, fig.height=5}
user32971_banana %>%
  na.omit() %>%
  group_by(days_since_prior_order) %>%
  summarise(count = n()) %>%
  ggplot(aes(x=count, y=days_since_prior_order)) +
  geom_boxplot(fill="seagreen4") +
  theme(axis.text.x=element_text(angle=45,hjust=1)) +
  labs(title="Days Since Prior Order",
       subtitle="for Banana Orders for User ID 32971",
       x="Days Since Prior Order",
       y="Count",
       fill="Day of the Week",
       caption="Data from InstaCart Kaggle Competition")
```


## Question 2 : Clustering

We'll start by looking at two methods to find *clusters* (subgroups) in the observations. As an example, we'll cluster `user_id` based on `reorder_products`, `num_products`, `days_since_prior_order`. That is, we'll use the values of these variables for each observation to define a notion of similarity between different observations (`user_id`) and then group similar observations.

### **Hierarchical Clustering**

We'll use *agglomerative* (bottom-up) clustering, where the main idea is to start with one leaf for each observation and then start joining pairs of "similar" leaves. This requires a notion of similarity (or dissimilarity). The default method of "complete" for `hclust()` computes the pairwise distance between every point in one cluster and every point in another cluster, and then takes the maximum of those distances as the distance between two clusters.

### 2a) Preparing the Data

# Make a `user_id`-based data table with potentially relevant variables
```{r}
uid_hc_train <-
  order_products %>%
  group_by(order_id) %>%
  summarise(num_products = n(),
            reorder_products = sum(reordered)) %>%
  left_join(orders, by="order_id") %>%
  filter(!is.na(days_since_prior_order), # omit NA's for `days_since_prior_order`
#         !days_since_prior_order == 30,  # omit `days_since_prior_order` = 30 since > 30 days are also      
                                         # categorized as 30 days which can cause discrepancy in the data
#         num_products >= 10,             # `num_products` greater than 10 to develop better algorithm
         eval_set=="train") %>%          # "train" data
  mutate(user_id=as.character(user_id)) %>%
  group_by(user_id, order_number) %>% 
  summarize(num_products = sum(num_products),
            reorder_products = sum(reorder_products),
            reord_ord_ratio = as.numeric(format(round(reorder_products/num_products, 3), nsmall=3)),
            ave_days_since = mean(days_since_prior_order)) %>%
  arrange(desc(num_products))
head(uid_hc_train, n=10)
```

# Inspection of the Data Table `uid_hc_train`

```{r}
# Inspect data table `uid_hc_train`
glimpse(uid_hc_train)
#View(uid_hc_train)


# No variable is missing any values
sum(is.na(uid_hc_train$user_id))
sum(is.na(uid_hc_train$order_number))
sum(is.na(uid_hc_train$num_products))
sum(is.na(uid_hc_train$reorder_products))
sum(is.na(uid_hc_train$ave_days_since))


# Variable Inspection
## `num_products` across `user_id`
min(uid_hc_train$num_products)
max(uid_hc_train$num_products)
mean(uid_hc_train$num_products)
sd(uid_hc_train$num_products)

## `reorder_products` across `user_id`
min(uid_hc_train$reorder_products)
max(uid_hc_train$reorder_products)
mean(uid_hc_train$reorder_products)
sd(uid_hc_train$reorder_products)

## `reord_ord_ratio` across `user_id`
mean(uid_hc_train$reord_ord_ratio)
sd(uid_hc_train$reord_ord_ratio)

## `ave_days_since` across `user_id`
min(uid_hc_train$ave_days_since)
max(uid_hc_train$ave_days_since)
mean(uid_hc_train$ave_days_since)
sd(uid_hc_train$ave_days_since)
```

### 2b) Processing Data for Clustering CHANGE !!!

#### Normalize
```{r}
order_norm <- normalize(uid_hc_train)
sapply(order_norm, mean)
sapply(order_norm, var)
```

#### Perform Hierarchical Clustering and Plot the Resulting Dendrogram:
```{r}
set.seed(15)
order_norm_diffs <- dist(order_norm)
```

```{r}
order_norm_diffs %>%
  hclust("ave")%>%
  as.phylo()%>%
  plot(cex=0.5,label.offset=1)

## cut at height == 100
d1 <- cut(as.dendrogram(hc), h=1)
d2 <- cut(as.dendrogram(hc), h=2)
## cut returns a list of sub-dendrograms
d1


par(mfrow=c(1, 2))
plot(d$lower[[1]])
plot(d$lower[[2]])
plot(d$lower[[3]])
par(mfrow=c(1, 1))
```

**k-Means**

Another extremely popular clustering method is the k-means algorithm. The basis idea is as follows. Assign all movies a random cluster ID. For each group, compute the geometric center, once again thinking of each movie as a vector in $R^N$, where $N$ is the number of features (6 in this case). Then go through and update the clusters by assigning each movie to the group whose centroid is closest to its feature vector. Repeat this process until convergence. One important feature (and sometimes drawback) of this algorithm is that we need to specify the number of clusters ahead of time. Another is that the final clustering depends on the random start, so it is common to repeat the process with different random starting points, and then selecting the best output (the one that minimizes an objective function). The good news is that it is very fast, and scales to large data sets.

Perform k-means clustering on non-normalized data:
```{r}
set.seed(1)
num_clusters<-8
kmean<-order_norm
order_clusts<-kmean%>%
  kmeans(centers=num_clusters)%>%
  fitted("classes")%>%
  as.character()
kmean<-kmean%>%
  mutate(cluster=order_clusts)
```

```{r,echo=FALSE}
ggplot(kmean,aes(x=ave_days_since,y=reorder_products))+
  geom_point(aes(color=cluster),alpha=.5)
```

As a test, do action comedies fall into the same cluster?

```{r,echo=FALSE}
# ggplot(kmean,aes(x=ave_days_since,y=reorder_products))+
#   geom_point(aes(color=cluster,alpha="Action"))
```

What if we use normalized data instead?
```{r}
set.seed(1)
order_clusts_norm<-order_norm%>%
  kmeans(centers=num_clusters)%>%
  fitted("classes")%>%
  as.character()
kmean<-kmean %>%
  mutate(cluster_norm=order_clusts_norm)
```

```{r,echo=FALSE}
ggplot(comedy_select,aes(x=numRatings,y=avgRating))+geom_point(aes(color=cluster_norm),alpha=.5)
```

Action comedies now fall almost entirely in one cluster:

```{r,echo=FALSE}
ggplot(comedy_select,aes(x=numRatings,y=avgRating))+geom_point(aes(color=cluster_norm,alpha=Action))
```

How about romantic comedies? 
```{r,echo=FALSE}
ggplot(comedy_select,aes(x=numRatings,y=avgRating))+geom_point(aes(color=cluster_norm,alpha=Romance))
```

Most fall into the same category, but some of the really popular and really unpopular ones are in different categories. 

Let's take a closer look at one of the categories to see if we can guess what they have in common?

```{r,fig.width=12,fig.height=8}
comedy_select_big<-left_join(comedy_select,comedy)
clust2<-filter(comedy_select_big,cluster_norm==2)
ggplot(clust2,aes(x=numRatings,y=avgRating))+geom_text(aes(label=movie_title),position=position_jitter())
```

Yes, those are all animated movies! In fact, they are all of the animated comedies.

## Principal Components Analysis (PCA)

The main idea of principal components analysis is as follows. When you have a large number of variables, it can be helpful to discover new "variables" that are in some ways combinations of the other variables. The goal is that just a few (say $m$) of these new combinations can explain much more of the variation in the data (i.e., capture key discriminating patterns) than any $m$ of the original variables. We can then project our data onto the lower dimensional subspace spanned by these first $m$ principal component vectors. 

In the following example, we'll analyze users based on their ratings for comedy movies. In particular, we'll look at their number of ratings and average ratings for (i) all comedies, (ii) action comedies, (iii) romantic comedies, (iv) dramatic comedies, and (v) animated comedies.

Tally numbers of ratings and compute mean rating for each category:
```{r}
comRatings<-Ratings%>%
  left_join(Movies)%>%
  filter(Comedy==TRUE)%>%
  select(user_id,rating,Romance,Action,Drama,Animation)%>%
  group_by(user_id)%>%
  summarise(numComRatings=n(),avgComRating=mean(rating),
            numRomCom=sum(Romance==1),
            avgRomComRating=ifelse(sum(Romance==1)==0,NA,mean(rating[Romance==1])),
            numActionCom=sum(Action==1),
            avgActionComRating=ifelse(sum(Action==1)==0,NA,mean(rating[Action==1])),
            numAnimaCom=sum(Animation==1),
            avgAnimaComRating=ifelse(sum(Animation==1)==0,NA,mean(rating[Animation==1])),
            numDramCom=sum(Drama==1),
            avgDramComRating=ifelse(sum(Drama==1)==0,NA,mean(rating[Drama==1]))
)
rownames(comRatings)<-comedy$user_id
```

For simplicity, omit any users who have not rated at least one comedy movie in each of the romance, action, animation, and drama categories:
```{r}
comRatingsSelect<-na.omit(comRatings)
```

Normalize data and perform PCA:
```{r}
normComRatings<-normalize(comRatingsSelect[,2:11])
pr.out=prcomp(normComRatings,center=TRUE,scale=TRUE) # we've already normalized data, but this would also do the trick, so it is good practice to leave the center and scale set to TRUE
```

We can now project the data down onto the first two principal component vectors. This is an example of *dimensionality reduction*! 
```{r,fig.width=12,fig.height=14}
pr.out$rotation
biplot(pr.out, scale=0)
```
My interpretation of this plot is as follows: The first principal component vector is highly correlated with all of the counts, and so, roughly speaking, this captures each user's overall activity level. The second principal component captures some notion of each user's rating difficulty.

Let's check out User 405, which seems to be an outlier, with an extreme value in the first principal component:
```{r}
filter(comRatings,user_id==405)
```
This person watched and rated many comedy movies, but does not seem to like many of them. Perhaps they liked Grumpy Old Men? Cha-ching.

How about Users 329 and 23? They seem to be an outliers at opposite end of the second principal component.
```{r}
filter(comRatings,user_id==329 | user_id==23)
```



How much of the variance is explained by each principle component?^[We can also write these quantities in terms of the eigenvalues of $AA^{T}$, where $A$ is our data matrix. For that, you'll have to take MATH/COMP 365.]
```{r}
pr.var=pr.out$sdev^2
(pve=pr.var/sum(pr.var))
```

So the first two principal components explain more than 64% of the variation in the data.


One final note: Oftentimes, you might want to first use a dimensionality reduction technique like PCA, and then use a clustering method like k-means to cluster in this lower dimensional space. That can actually help reduce some of the noise in the data before clustering.

# Homework Assignment

Test out some of these unsupervised machine learning techniques on your project data. Each person should conduct their own analyses on different topics. How does normalizing your data affect the results? What structure can you find in the data?  Try to learn a model that provides some insights, and demonstrate these insights with a visualization. Once you've explored a number of different methods and models, choose **two** to turn in. For each one, include:

- the research question
- the method you chose, and why you chose it
- an illustrative visualization (not just the resulting tree, although you can include that)
- some insights that came out of your analysis






















































## User with the highest order

### Prior
```{r}
top_order_userid_prior <-
  order_products %>%
  group_by(order_id) %>%
  summarise(num_products = n(),
            reorder_products = sum(reordered)) %>%
  left_join(orders, by="order_id") %>%
  filter(!is.na(days_since_prior_order),
         eval_set=="prior") %>%
  group_by(user_id) %>%
  summarize(num_products = sum(num_products),
            reorder_products = sum(reorder_products),
            ave_days_since = mean(days_since_prior_order),
            reord_ord_ratio = reorder_products/num_products) %>%
  arrange(desc(num_products))
head(top_order_userid_prior)
```

### Train
```{r}
top_order_userid_train <-
  order_products %>%
  group_by(order_id) %>%
  summarise(num_products = n(),
            reorder_products = sum(reordered)) %>%
  left_join(orders, by="order_id") %>%
  filter(!is.na(days_since_prior_order),
         eval_set=="train") %>%
  group_by(user_id) %>%
  summarize(num_products = sum(num_products),
            reorder_products = sum(reorder_products),
            ave_days_since = mean(days_since_prior_order),
            reord_ord_ratio = reorder_products/num_products) %>%
  arrange(desc(num_products))
head(top_order_userid_train)
```

```{r}
names(top_order_userid_train)
```


# Predictions
## Q: Predict number of reorders per each user

## Regression Tree

I chose regression tree because tree is an easy and simple regression and classification method to use.

The tree below tells us that both `num_products` and `reord_order_ratio` are good candidates for predicting `reorder_products`. This makes sense since `num_products` is the denominator of `reord_order_ratio`. One indicator is that `ave_days_since` is not so predictive of `reorder_products` possibly because of missing variables.

I would like to do a filtered-by-a- `user_id` analysis after this file for further predictive analysis.

```{r echo=FALSE}
set.seed(135)
reg_tree <- tree(reorder_products ~ num_products + ave_days_since + reord_ord_ratio, 
                 data = top_order_userid_train)
summary(reg_tree)
plot(reg_tree)
text(reg_tree, pretty=1)
```

### Visualization: Orders and Reorders by Average Days Since Last Order 

```{r}
top_order_userid_train %>%
  na.omit() %>%
  ggplot(aes(x=num_products, y=reorder_products, color=ave_days_since, alpha=0.15)) + 
  geom_point() + 
  theme(axis.text.x=element_text(angle=45,hjust=1)) +
  labs(x="Number of Ordered Products", y="Number of Reordered Products",
       title="Orders and Reorders by User ID",  
       color="Average Days Since Last Order",
       alpha="Transparency",
       caption="Data from InstaCart Kaggle Competition")
```
It seems that the lower the `reord_ord_ratio`, the longer it takes for you to make an order again. However, this is not captured in the basic tree analysis.




## Gradient Boosted Tree

I chose gradient boosted tree because I have personally never used this method before, and wanted to explore the difference in outputs between other regression methods and GBT. Again, it seems that the variables in the data except for those that are directly related (`reord_ord_ratio` and `num_products`) do not really contribute to predicting `reorder_products`. As with the first analysis, `ave_days_since` does not explain the variability in `reorder_products` well.

```{r}
set.seed(345)
grad_boost = gbm(reorder_products ~ num_products + ave_days_since + reord_ord_ratio + user_id, 
                 data = top_order_userid_train,
              distribution = "gaussian", n.trees = 100,
              interaction.depth = 4)
summary(grad_boost) # Analyze variable importance
```

```{r}
plot(grad_boost, i = c("reord_ord_ratio","num_products")) 
```

Again, I need to create more variables for better analysis. These files are in progress as of now. 

```{r}
tree_test <- predict(reg_tree, top_order_userid_prior)
grad_boost_test <- predict(grad_boost, top_order_userid_prior, n.trees=100)    

train_results <- top_order_userid_prior %>% 
    mutate(tree_test, grad_boost_test)%>%
    mutate(tree_error=(reorder_products-tree_test), 
           gb_error=(reorder_products-grad_boost_test))
mean(train_results$tree_error^2) 
mean(train_results$gb_error^2) 
```

Tree has a lower mean squared.

```{r}
# save.image(file = "my_work_space.RData")
```