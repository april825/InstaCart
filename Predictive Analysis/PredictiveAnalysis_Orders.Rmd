---
title: "InstaCart Predictive Analysis: Predicting Orders"
author: "April Leclair, Daniel Ochoa, Hoang Anh Thai Vu, Qiuhan Sun"
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "PredictiveAnalysis_HTML_Output") })
output:
  bookdown::tufte_html2:
    number_sections: no
    split_by: none
    toc: no
  bookdown::html_document2:
    number_sections: no
    split_by: none
    toc: no
  bookdown::tufte_handout2:
    latex_engine: xelatex
    number_sections: no
    toc: no
---

```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(xgboost)
library(caret)  # Used for confusion matrix
library(Ckmeans.1d.dp)  # Used for XGBoost visualization
library(DiagrammeR)  # Used for XGBoost visualization
```

# Reading In The Data
```{r message=FALSE, warning=FALSE, echo=FALSE}
trainDF <- read_csv("../Source/trainingData.csv")
# test <- read_csv("../Source/testingData.csv")  # Only to be used when turning in to kaggle competition
products <- read_csv("../Source/products.csv")  # Used to provide product names (hasn't been incorporated yet)
```

```{r}
trainDF
```

Since we don't have the answers to the test dataset we need to partition our training set into training and testing to validate our results and avoid overfitting.
```{r}
set.seed(567)

inTrain <- sample_frac(data.frame(unique(trainDF$order_id)), 0.7)

train <- trainDF %>%
  filter(order_id %in% inTrain$unique.trainDF.order_id.)
test <- trainDF %>%
  filter(!order_id %in% inTrain$unique.trainDF.order_id.)

rm(trainDF)
rm(inTrain)
```

For model training we are going to want to seperate the independent variables from the response variable.
```{r}
trainIndependents <- train %>% 
  select(-reordered, -product_id, -order_id, -user_id)

testIndependents <- test %>% 
  select(-reordered,  -product_id, -order_id, -user_id)
```

DONT DELETE: FOR BLOG/PRESENTATION
Parameters that we have found to hurt the model more than aid (noise): product_id, order_id, product.avgCartPosition, user_product.avgCartPosition, -user_product.avgHourOfDayDifference, -user.avgOrderSize, -user.numOrders)

# Predictions

## Accuracy Measurment: F-1 Score

The standard measurement of accuracy for binary classification is F-1 score. It considers both the `precision` and the `recall` rate of the prediction: `precision` is the number of correct positive results divided by the number of all positive results, and `recall` is the number of correct positive results divided by the number of true positive results. Source: Wikipedia

We define a function that takes the predictions and references as input, and returns the confusion matrix, precision, recall, and F-1 Score

```{r}
accuracy.Test <- function(prediction,reference){
  con <- confusionMatrix(prediction,reference)
  matrix <- con$table
  precision <- matrix[2,2]/sum(matrix[,2])
  recall <- matrix[2,2]/sum(matrix[2,])
  f1 <- 2*precision*recall/(precision+recall)
  stat=list(precision=precision,recall=recall,f1=f1)
  result=list(matrix=matrix,stat=stat)
  return(result)
}
```

## Null Model 

We are going to want to measure the performance of our prediction against some naive model. A simple model would be the following: If a user orderered this product last time, they will order against this time.

Below we see how such a model performs.
```{r}
nullPredict <- ifelse(test$user_product.order_streak > 0, 1, 0)
accuracy.Test(nullPredict,test$reordered)
```

## Gradient Boosted Tree Model 

```{r}
trainingMatrix <- xgb.DMatrix(as.matrix(trainIndependents), label = train$reordered)
testMatrix <- xgb.DMatrix(as.matrix(testIndependents), label = test$reordered)
```

## Parameter Tuning

TODO: Tune parameters!
```{r}
params <- list("objective" = "binary:logistic",
               "max_depth" = 6,
               "eta" = 0.1,
               "min_child_weight" = 10,
               "subsample" = 0.8)
```

Now to use a 4-fold cross validation to narrow in on the number of iterations to minimize error rate.

Still improving after 130 iterations, need to use more! Marginal improvement tho
```{r}
cv <- xgb.cv(data = trainingMatrix, nfold=4, param=params, nrounds=200, early_stopping_rounds=20, print_every_n = 10)
```

```{r}
model <- xgb.train(data = trainingMatrix, param=params, nrounds=130, verbose=FALSE)
importance <- xgb.importance(colnames(trainingMatrix), model = model)
xgb.ggplot.importance(importance)
```

Looking at the stats of the feature importance
```{r}
importance
```

```{r}
xgbpred <- predict(model, testMatrix)
xgbpred <- ifelse(xgbpred > 0.1, 1, 0) # 0.1 is threshold I came up with after messing around, experiment with it
confusionMatrix(xgbpred, test$reordered)
accuracy.Test(xgbpred,test$reordered)
```

We can see from this which features are the most important. It is no suprise that user_product.orders is very important but we were surpised to find that user.numOrders is very important too. We will investigate this in the future.

```{r eval=FALSE}
xgb.plot.tree(feature_names=colnames(importance), model=model, n_first_tree=1)
```
