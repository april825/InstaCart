---
title: "InstaCart Predictive Analysis: Predicting Orders"
author: "April Leclair, Daniel Ochoa, Hoang Anh Thai Vu, Qiuhan Sun"
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "PredictiveAnalysis_HTML_Output") })
output:
  bookdown::tufte_html2:
    number_sections: no
    split_by: none
    toc: no
  bookdown::html_document2:
    number_sections: no
    split_by: none
    toc: no
  bookdown::tufte_handout2:
    latex_engine: xelatex
    number_sections: no
    toc: no
---

```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(xgboost)
library(caret)  # Used for confusion matrix
library(Ckmeans.1d.dp)  # Used for XGBoost visualization
library(DiagrammeR)  # Used for XGBoost visualization
library(ggplot2)
```

# Reading In The Data
```{r message=FALSE, warning=FALSE, echo=FALSE}
trainDF <- read_csv("../Source/trainingData.csv")
# test <- read_csv("../Source/testingData.csv")  # Only to be used when turning in to kaggle competition
products <- read_csv("../Source/products.csv")  # Used to provide product names (hasn't been incorporated yet)
```

Since we don't have the answers to the test dataset we need to partition our training set into training and testing to validate our results and avoid overfitting.
```{r}
set.seed(567) # Used for reproducability of results

inTrain <- sample_frac(data.frame(unique(trainDF$order_id)), 0.7)

train <- trainDF %>%
  filter(order_id %in% inTrain$unique.trainDF.order_id.)
test <- trainDF %>%
  filter(!order_id %in% inTrain$unique.trainDF.order_id.)

rm(trainDF)
rm(inTrain)
```

For model training we are going to want to seperate the independent variables from the response variable.
```{r}
trainIndependents <- train %>% 
  select(-reordered, -product_id, -order_id, -user_id)

testIndependents <- test %>% 
  select(-reordered,  -product_id, -order_id, -user_id)
```

DONT DELETE: FOR BLOG/PRESENTATION
Parameters that we have found to hurt the model more than aid (noise): product_id, order_id, product.avgCartPosition, user_product.avgCartPosition, -user_product.avgHourOfDayDifference, -user.avgOrderSize, -user.numOrders)

# Predictions

## Accuracy Measurment: F-1 Score

The standard measurement of accuracy for binary classification is F-1 score. It considers both the `precision` and the `recall` rate of the prediction: `precision` is the number of correct positive results divided by the number of all positive results, and `recall` is the number of correct positive results divided by the number of true positive results. Source: Wikipedia

We define a function that takes the predictions and references as input, and returns the confusion matrix, precision, recall, and F-1 Score

```{r}
# Testing function
f1_test <- function (pred, ref,user_id) {
  require(ModelMetrics)
  dt <- tibble(user_id, pred, ref)
  dt <- dt %>%
    group_by(user_id)%>%
    mutate(f1_score = f1Score(pred,ref))%>%
    summarise(f1_score = mean(f1_score,na.rm=TRUE))
  f1_mean <- mean(dt$f1_score,na.rm=TRUE)
  return (f1_mean)
}
```

## Null Model 

We are going to want to measure the performance of our prediction against some naive model. A simple model would be the following: If a user orderered this product last time, they will order against this time.

Below we see how such a model performs.
```{r}
nullPredict <- ifelse(test$user_product.order_streak > 0, 1, 0)
f1_test(nullPredict,test$reordered,test$user_id)
```

## Gradient Boosted Tree Model 

```{}
# Evaluation function
xgb_eval_f1 <- function (yhat, dtrain) {
  require(ModelMetrics)
  y = getinfo(dtrain, "label")
  dt <- data.table(user_id=train[user_id %in% val_users, user_id], purch=y, pred=yhat)
  f1 <- mean(dt[,.(f1score=f1Score(purch, pred, cutoff=0.249)), by=user_id]$f1score)
  return (list(metric = "f1", value = f1))
}
```

```{r}
trainingMatrix <- xgb.DMatrix(as.matrix(trainIndependents), label = train$reordered)
testMatrix <- xgb.DMatrix(as.matrix(testIndependents), label = test$reordered)
```

## Parameter Tuning

```{r}
params <- list(objective = "binary:logistic",
               max_depth = 6,
               eta = 0.1,
               min_child_weight = 10,
               subsample = 0.8)
```


Now to use a 4-fold cross validation to narrow in on the number of iterations to minimize error rate.
```{r eval=FALSE}
cv <- xgb.cv(data = trainingMatrix, nfold=4, param=params, nrounds=200, early_stopping_rounds=20, print_every_n = 10)
```

Now to plot the error
```{r eval=FALSE}
ggplot(data=cv$evaluation_log, aes(x=iter, y=test_error_mean)) + geom_line() + geom_point(x=cv$best_iteration, y=min(cv$evaluation_log$test_error_mean), color="red") + labs(title="Optimal nrounds", subtitle="Using Binary Classification Error Rate", x="nrounds", y="Test Error Mean")
```

Below we do a manual grid search for fine tuning hyperparameters (max_depth and min_child_weight in this example). Note that this is very computationally expensive and will take quite some time.
```{r, eval=FALSE}
grid <- expand.grid(
  nrounds = 200,
  colsample_bytree = 1,
  gamma = 1,
  eta = 0.1,
  subsample = 0.8,
  max_depth = c(5,6),
  min_child_weight = c(8,10)
)

# Have to create new labeled reordered column because 0 and 1 are not valid names.
gridSearchLabels <- train$reordered
gridSearchLabels[gridSearchLabels==0] <- 'no'
gridSearchLabels[gridSearchLabels ==1] <- 'yes'

control <- trainControl(method="cv", number=4, verboseIter = TRUE, returnData = FALSE, returnResamp = "all", classProbs = TRUE, summaryFunction = twoClassSummary, allowParallel = TRUE)
gridModel = train(x=as.matrix(trainIndependents), y=as.factor(gridSearchLabels),
trControl=control,
tuneGrid=grid,
method="xgbTree"
)
```

Now to plot reults and find the best parameters 
```{r, eval=FALSE}
ggplot(gridModel$results, aes(x=as.factor(max_depth), y = min_child_weight, size = ROC, color = ROC)) + geom_point() +
theme_bw() + scale_size_continuous(guide = "none")
gridModel$bestTune
```

## Predictions
```{r}
model <- xgb.train(data = trainingMatrix, param=params, nrounds=200, verbose=FALSE)
importance <- xgb.importance(colnames(trainingMatrix), model = model)
xgb.ggplot.importance(importance)
```

We can see from this which features are the most important.

# Finding Threshold for Classification

```{r, warning=FALSE}
# Finding threshold
xgbpred_train <- predict(model,trainingMatrix)
kk=seq(0.01,0.99,length=100)
threshold = c()
for (k in kk){
  xgbpred_k <- ifelse(xgbpred_train > k, 1, 0)
  threshold <- c(threshold,f1_test(xgbpred_k,train$reordered,train$user_id))
}
cutoff <- kk[which.max(threshold)]
ggplot() + geom_point(aes(kk,threshold)) + geom_point(aes(x=cutoff,y=max(threshold, na.rm = TRUE)), color="red") + labs(title="Errors with Different Threshold", y="Mean F1 Score",x="Threshold")
```

```{r, warning=FALSE}
ggplot() + geom_point(aes(kk,threshold)) + geom_point(aes(x=cutoff,y=max(threshold, na.rm = TRUE)), color="red") + labs(title="Errors with Different Threshold", y="Mean F1 Score",x="Threshold")
```

```{r}
xgbpred <- predict(model, testMatrix)
xgbpred <- ifelse(xgbpred > cutoff, 1, 0) 
confusionMatrix(xgbpred, test$reordered)
message("F1 Score")
f1_test(xgbpred, test$reordered,test$user_id)
```

## Prediction Comparisons and Visualizations

We are going to build a table that has both all of the products that a order was actually comprised of and all the products that the model predicted it would be comprised of.

Begin with the predictions
```{r message=FALSE}
# List of items predicted to be ordered in each order
predicted_products <- test %>%
  mutate(prediction = xgbpred) %>%
  group_by(order_id) %>%
  filter(prediction == 1) %>%
  summarise(pred.products = paste(product_id, collapse = ", "))

missing <- data.frame(
  order_id = unique(test$order_id[!test$order_id %in% predicted_products$order_id]),
  pred.products = "None"
)

predicted_products <- predicted_products %>%
  bind_rows(missing) %>%
  arrange(order_id)

rm(missing)
```

Now the actual orders and then joining with the predictions.
```{r message=FALSE}
# List of items actually ordered in each order
products_comparison <- test %>%
  group_by(order_id) %>%
  filter(reordered == 1) %>%
  summarise(actual.products = paste(product_id, collapse = ", "))

missing <- data.frame(
  order_id = unique(test$order_id[!test$order_id %in% products_comparison$order_id]),
  actual.products = "None"
)

(products_comparison <- products_comparison %>%
  bind_rows(missing) %>%
  left_join(predicted_products, by="order_id") %>%
  arrange(order_id))

rm(missing)
```

Below we can look at the tree that the model uses to make its predictions.
```{r eval=FALSE}
xgb.plot.tree(feature_names=colnames(importance), model=model, n_first_tree=1)
```
