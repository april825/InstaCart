---
title: "InstaCart Predictive Data Prep: Logit with `clust4_Prod`"
author: "April Leclair, Daniel Ochoa, Hoang Anh Thai Vu, Qiuhan Sun"
date: "December 11, 2017"
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "PredictiveAnalysis_HTML_Output") })
output:
  bookdown::tufte_html2:
    number_sections: no
    split_by: none
    toc: no
  bookdown::html_document2:
    number_sections: no
    split_by: none
    toc: no
  bookdown::tufte_handout2:
    latex_engine: xelatex
    number_sections: no
    toc: no
---

```{r setup, include=FALSE, cache=FALSE}
library(tidyverse)
library(readr)
library(pROC)
knitr::opts_chunk$set(tidy = FALSE, message=FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```



# Prepare the Data

## Load the Data

```{r message=FALSE, warning=FALSE, cache=FALSE}
load("../Source/data_final.Rda")
load("../Source/clust4_Prod.Rda")
```


```{r}
df.clust4_Prod <- subset(data_final, product_id %in% clust4_Prod)  
```


```{r}
anyNA(df.clust4_Prod)
length(unique(data_final$product_id)) # 
length(unique(df.clust4_Prod$product_id))==length(clust4_Prod)
length(unique(df.clust4_Prod$product_id))
length(unique(df.clust4_Prod$user_id))
```


## Train & Test

```{r cache.lazy=FALSE}
samplesize <- floor(0.75 * nrow(df.clust4_Prod))
set.seed(098)
all_indices = c(1:nrow(df.clust4_Prod))
train_indices = sample(seq_len(nrow(df.clust4_Prod)), size = samplesize)
test_indices = setdiff(all_indices, train_indices)
all( ! test_indices %in% train_indices)

# Train_data
train = df.clust4_Prod[train_indices, ]
train <- train %>% arrange(user_id, product_id)

# Test_data
test = df.clust4_Prod[test_indices, ] 
test <- test %>% arrange(user_id, product_id)

rm(samplesize, all_indices, train_indices, test_indices, df.clust4_Prod)
```

## Train & Test - processing for model prediction later

The Number of users have to match, so we do some processing for train and test.

```{r}
anyNA(test)
length(unique(test$product_id))
length(unique(train$product_id))

train2 <- subset(train, user_id %in% unique(test$user_id))
test2 <- subset(test, user_id %in% unique(train2$user_id))

length(unique(train2$user_id))
length(unique(test2$user_id))

train <- train2
test <- test2

nrow(test)/nrow(train) # test:train ratio is still around 70%
rm(train2, test2)
```


# Predictive Analysis: Logistic Regression

## Full Model 

Fit the model for binomial regression.

```{r}
modelf <- glm(reordered ~ . - `user_id` - `product_id`, family = binomial(link = 'logit'), data = train)
```

Obtain the results of our model:
```{r}
summary(modelf)
```

## Fewer Variables

Fit the model for binomial regression. We remove these statistically insignificant variables this time.

```{r}
model2 <- glm(reordered ~ . - user_id - product_id - uid.numProducts - uid.reorderProducts - uid.maxDaysSince - uid.aveHr - uid.sdHr - uid.aveDow - uid.sdDow - uid.pantry_distr - uid.bakery_distr, family = binomial(link = 'logit'), data = train)
```

Obtain the results of our model:
```{r}
summary(model2)
```

## Even Fewer Variables

Remove variables with one star from `model2`

```{r}
model3 <- glm(reordered ~  . - user_id - product_id - uid.numProducts - uid.reorderProducts - uid.maxDaysSince - uid.aveHr - uid.sdHr - uid.aveDow - uid.sdDow - uid.pantry_distr - uid.snacks_distr - uid.frozen_distr - uid.bakery_distr, family = binomial(link = 'logit'), data = train)
```


Obtain the results of our model:
```{r}
summary(model3)
```

## Compare Models

```{r}
anova(modelf, model2, test="Chisq")
anova(modelf, model3, test="Chisq")
anova(model2, model3, test="Chisq")
```

All are significantly different for each pair. However, "modelf" and model3" has the strongest significant difference at p = 0.01.


## Testing Function

```{r}
accuracy_test <- function(pred_mod) {
  
  # confusion matrix, precision, recall
  cmt <- confusionMatrix(pred_mod, test$reordered)
  recall <- cmt[2,2]/sum(cmt[2,])
  precision <- cmt[2,2]/sum(cmt[,2])
  specificity <- cmt[1,1]/sum(cmt[1,])
  accuracy <- (cmt[2,2]+cmt[1,1])/(cmt[1,1]+cmt[2,2]+cmt[2,1]+cmt[1,2])
  
  # print output
  result=list(confMatrTable = unlist(cmt), precision=precision, recall=recall,
              specificity=specificity, accuracy=accuracy)
  
  return(result)
}
  
  
  
f1_test <- function (pred, ref,user_id) {
  require(ModelMetrics)
  dt <- tibble(user_id, pred, ref)
  dt <- dt %>%
    group_by(user_id)%>%
    mutate(f1_score = f1Score(pred,ref))%>%
    summarise(f1_score = mean(f1_score,na.rm=TRUE))
  f1_mean <- mean(dt$f1_score,na.rm=TRUE)
  return (f1_mean)
}



f1_comp_test <- function(model, cutoff){

  # prediction for our model
  pred_mod_tem <- predict(model, newdata = test, type = 'response')
  pred_mod <- ifelse(pred_mod_tem > cutoff, 1, 0)
  pred_mod[is.na(pred_mod)]<-0
  pred_f1 <- f1_test(pred_mod, test$reordered, test$user_id)
  
  # prediction for null model
  pred_null <- ifelse(test$user_product.order_streak > 0, 1, 0)
  null_f1 <- f1_test(pred_null, test$reordered, test$user_id)
  
  # comparison to null model
  diff_raw <- pred_f1-null_f1
  diff_perc <- diff_raw/null_f1
  
  
  acc_output <- accuracy_test(pred_mod)
  
  # print output
  result1=list(pred_f1=pred_f1)
  result2=list(null_f1=null_f1)
  result3=list(diff_raw=diff_raw, diff_perc=diff_perc)
  result4=acc_output
  
  return(c(result1, result2, result3, result4))
}
```


## Results

Threshold that maximizes F1.

```{r}
f1_comp_test(model3, 0.84)
```


## Plot GLM

```{r warning = FALSE}
pred_logit_pre <- predict(model3, newdata = test, type = 'response')

ggplot(test, aes(x=pred_logit_pre, y=reordered)) + geom_point() +
  stat_smooth(method="glm", family="binomial", se=TRUE) +
  labs(x="Prediction", y="Actual",
       title="Logistic Regression of Prediction vs. Actual Reordered",
       caption="Data from InstaCart Kaggle Competition")
```


## ROC

Threshold that maximizes AUC.

```{r warning = FALSE}
pred_logit <- ifelse(pred_logit_pre > 0.36, 1, 0)
roc(test$reordered, pred_logit, plot=TRUE)
```